
<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Hao Lu</title>
  
  <meta name="author" content="Hao Lu">
  <meta name="viewport" content="width=device-width, initial-scale=1">  
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hao Lu</name>
              </p>
              <p> 
                I am currently a Ph.D.
              </p>
              <p>
              I am generally interested in artificial intelligence and deep learning. My current research focuses on:
              </p>
              <p>ðŸ¦™ <b style="color:brown">Large Models</b> + ðŸš™ <b style="color:green">Embodied Agents</b> ->  ðŸ¤– <b style="color:orange">AGI</b> </p>
              <li style="margin: 5px;">
                ðŸ¦™ <b style="color:brown">Large Models and World Models</b>: Efficient/Small LLMs, Multimodal Models, Video Generation Models, Large Action Models...
              </li>
              <li style="margin: 5px;">
                ðŸš™ <b style="color:green">Embodied Agents and Spatial Intelligence</b>: 3D Occupancy Prediction, End-to-End Driving, 3D Scene Reconstruction, 4D Scene Simulation...
              </li>
              </p>
              <b>If you're interested or have questions about my research, feel free to drop me an email at hlu585@connect.hkust-gz.edu.cn.</b>
              <p style="text-align:center">
                <a href="mailto:hlu585@connect.hkust-gz.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?user=OrbGCGkAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/haolu.png">
            </td>
          </tr>
        </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      <td style="text-indent:20px;width:100%;vertical-align:middle">
      <p>
        *Equal contribution &nbsp;&nbsp;.
      </p>
      </td>
    </tbody></table>

	
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Papers <a href="https://scholar.google.com/citations?hl=zh-CN&user=LdK9scgAAAAJ&view_op=list_works&sortby=pubdate"> [Full List] </a></heading></p>
              <!-- <p>
                *Equal contribution &nbsp;&nbsp; <sup>â€ </sup>Project leader/Corresponding author.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          

          <h3 style="text-indent:20px;color:brown">World Models</h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Owl.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Owl-1: Omni World Model for Consistent Long Video Generation</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <strong>Hao Lu<sup>â€ </sup></strong>, 
              <a href=""> Yuan Gao </a>,  
              <a href=""> Xin Tao </a>,  
              <a href=""> Pengfei Wan </a>,  
              <a href=""> Di Zhang </a>,  
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2412.09600">[arXiv]</a> 
              <a href="https://github.com/huang-yh/Owl">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=Owl&type=star&count=true" >
              </iframe>
              <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
              <a href="https://www.jiqizhixin.com/articles/2025-01-15">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> Owl-1 approaches consistent long video generation with an omni world model, which models the evolution of the underlying world with latent state, explicit observation and world dynamics variables.  </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OccWorld.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</papertitle>
              <br> 
              <strong>Hao Lu*<sup>â€ </sup></strong>, 
              <a href="https://github.com/chen-wl20"> Weiliang Chen* </a>,  
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="http://boruizhang.site/"> Borui Zhang </a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan</a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.16038">[arXiv]</a> 
              <a href="https://github.com/wzzheng/OccWorld">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=OccWorld&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/OccWorld/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/669979822">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> OccWorld models the joint evolutions of 3D scenes and ego movements and paves the way for interpretable end-to-end large driving models.  </p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:brown">Efficient Large Models</h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SparseVLM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference</papertitle>
              <br> 
              <a href="https://gumpest.github.io/">Yuan Zhang*</a>,  
              <a href="https://scholar.google.com/citations?user=TxeAbWkAAAAJ">Chun-Kai Fan*</a>,  
              <a href="">Junpeng Ma*</a>,  
              <strong>Hao Lu<sup>â€ </sup></strong>, 
              <a href="https://taohuang.info/">Tao Huang</a>,  
              <a href="https://cfcs.pku.edu.cn/people/faculty/kuancheng/index.htm">Kuan Cheng</a>,  
              <a href="https://gudovskiy.github.io/">Denis Gudovskiy</a>,  
              <a href="">Tomoyuki Okuno</a>,  
              <a href="">Yohei Nakata</a>,  
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer</a>,
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2410.04417">[arXiv]</a> 
              <a href="https://github.com/Gumpest/SparseVLMs">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=Gumpest&repo=SparseVLMs&type=star&count=true" >
              </iframe>
              <a href="https://leofan90.github.io/SparseVLMs.github.io/">[Project Page]</a>
              <!-- <a href="https://www.jiqizhixin.com/articles/2025-01-15">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p>SparseVLM sparsifies visual tokens adaptively based on the question prompt.</p>
            </td>
          </tr>



        </tbody></table>

      


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      
        <h3 style="text-indent:20px;color:green">Large Driving Models<a href="https://github.com/wzzheng/LDM"> [Page]</a></h3>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Doe-1.gif" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Doe-1: Closed-Loop Autonomous Driving with Large World Model</papertitle>
            <br> 
            <strong>Hao Lu*<sup>â€ </sup></strong>, 
            <a href="">Zetian Xia*</a>,  
            <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.09627">[arXiv]</a> 
            <a href="https://github.com/wzzheng/Doe">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=Doe&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Doe/">[Project Page]</a>
            <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
            <br>
            <p> Doe-1 is the first closed-loop autonomous driving model for unified perception, prediction, and planning.  </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/GPD-1.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>GPD-1: Generative Pre-training for Driving</papertitle>
            <br> 
            <a href="">Zetian Xia*</a>,  
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo*</a>, 
            <strong>Hao Lu*<sup>â€ </sup></strong>, 
            <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, 
            <a href=""> Dalong Du</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>,
            <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.08643">[arXiv]</a> 
            <a href="https://github.com/wzzheng/GPD">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GPD&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/GPD/">[Project Page]</a>
            <a href="https://zhuanlan.zhihu.com/p/17035347215">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
            <br>
            <p> GPD-1 proposes a unified approach that seamlessly accomplishes multiple aspects of scene evolution, including scene simulation, traffic simulation, closed-loop simulation, map prediction, and motion planning, all without additional fine-tuning.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="./images/Stag.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model</papertitle>
            <br> 
            <a href="https://github.com/LeningWang"> Lening Wang* </a>, 
            <strong>Hao Lu*<sup>â€ </sup></strong>, 
            <a href=""> Dalong Du</a>, 
            <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang</a>, 
            <a href="https://shi.buaa.edu.cn/renyilong/zh_CN/index.htm"> Yilong Ren </a>, 
            <a href="https://scholar.google.com/citations?user=d0WJTQgAAAAJ&amp;hl"> Han Jiang </a>, 
            <a href="https://zhiyongcui.com/"> Zhiyong Cui </a>, 
            <a href="https://shi.buaa.edu.cn/09558/zh_CN/index.htm"> Haiyang Yu </a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>,
            <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.05280">[arXiv]</a> 
            <a href="https://github.com/wzzheng/Stag">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=Stag&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Stag/">[Project Page]</a>
            <br>
            <p> Spatial-Temporal simulAtion for drivinG (Stag-1) enables controllable 4D autonomous driving simulation with spatial-temporal decoupling.  </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="./images/Driv3R.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving</papertitle>
            <br> 
            <a href="https://scholar.google.com/citations?user=r9rsD_0AAAAJ&hl=zh-CN&oi=sra"> Xin Fei </a>, 
            <strong>Hao Lu<sup>â€ </sup></strong>, 
            <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
            <a href="https://zhanwei.site/"> Wei Zhan </a>,
            <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>,
            <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>,
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.06777">[arXiv]</a> 
            <a href="https://github.com/Barrybarry-Smith/Driv3R">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=Barrybarry-Smith&repo=Driv3R&type=star&count=true" >
            </iframe>
            <a href="https://wzzheng.net/Driv3R/">[Project Page]</a>
            <br>
            <p> Driv3R predicts per-frame pointmaps in the global consistent coordinate system in an optimization-free manner. </p>
          </td>
        </tr>


      </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
          
            <h3 style="text-indent:20px;color:green">End-to-End Autonomou Driving</h3>





            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/GaussianAD.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>GaussianAD: Gaussian-Centric End-to-End Autonomous Driving</papertitle>
                <br>  
                <strong>Hao Lu*<sup>â€ </sup></strong>, 
                <a href=""> Junjie Wu*</a>,
                <a href=""> Yao Zheng*</a>,
                <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>,
                <a href=""> Zixun Xie</a>,
                <a href=""> Longchao Yang</a>,
                <a href=""> Yong Pan</a>,
                <a href=""> Zhihui Hao</a>,
                <a href=""> Peng Jia</a>,
                <a href=""> Xianpeng Lang</a>,
                <a href="https://www.shanghangzhang.com/"> Shanghang Zhang</a>
                <br>
                <em><strong>arXiv</strong></em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2412.10371">[arXiv]</a> 
                <a href="https://github.com/wzzheng/GaussianAD">[Code]</a>
                <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GaussianAD&type=star&count=true" >
                </iframe>
                <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
                <a href="https://zhuanlan.zhihu.com/p/16608177027">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
                <br>
                <p> GaussianAD is a Gaussian-centric end-to-end framework which employs sparse yet comprehensive 3D Gaussians to pass information through the pipeline to efficiently preserve more details.</p>
              </td>
            </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GenAD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GenAD: Generative End-to-End Autonomous Driving</papertitle>
              <br> 
              <strong>Hao Lu*</strong>, 
              <a href="https://github.com/songruiqi"> Ruiqi Song* </a>,  
              <a href="https://scholar.google.com/citations?user=jPvOqgYAAAAJ"> Xianda Guo* </a>, 
              <a href=""> Chenming Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=jzvXnkcAAAAJ"> Long Chen</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2402.11502">[arXiv]</a> 
              <a href="https://github.com/wzzheng/GenAD">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=GenAD&type=star&count=true" >
              </iframe>
              <a href="https://zhuanlan.zhihu.com/p/683302211">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GenAD casts end-to-end autonomous driving as a generative modeling problem.  </p>
            </td>
          </tr>


        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
            <h3 style="text-indent:20px;color:green">3D Occupancy Prediction</h3>



            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/GaussianWorld.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction</papertitle>
                <br>  
                <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo*</a>,
                <strong>Hao Lu*<sup>â€ </sup></strong>, 
                <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
                <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
                <br>
                <em><strong>arXiv</strong></em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2412.10373">[arXiv]</a> 
                <a href="https://github.com/zuosc19/GaussianWorld">[Code]</a>
                <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=zuosc19&repo=GaussianWorld&type=star&count=true" >
                </iframe>
                <!-- <a href="https://wzzheng.net/OccWorld/">[Project Page]</a> -->
                <!-- <a href="https://zhuanlan.zhihu.com/p/16608177027">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
                <br>
                <p> GaussianWorld reformulates 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input and propose a Gaussian World Model to exploit the scene evolution for perception.</p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GaussianFormer.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <strong>Hao Lu<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong></em>), 2024.
              <br>
              <a href="https://arxiv.org/abs/2405.17429">[arXiv]</a> 
              <a href="https://github.com/huang-yh/GaussianFormer">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=GaussianFormer&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/GaussianFormer/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/700833107">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> GaussianFormer proposes the 3D semantic Gaussians as <b>a more efficient object-centric</b> representation for driving scenes compared with 3D occupancy.  </p>
            </td>
          </tr>     

          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SelfOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Hao Lu*<sup>â€ </sup></strong>, 
              <a href="http://boruizhang.site/"> Borui Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2311.12754">[arXiv]</a>
              <a href="https://github.com/huang-yh/SelfOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=huang-yh&repo=SelfOcc&type=star&count=true" >
              </iframe>
              <a href="https://huang-yh.github.io/SelfOcc/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/677380563">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> SelfOcc is the first self-supervised work that produces reasonable 3D occupancy for surround cameras. </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/tpvformer.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang* </a>, 
              <strong>Hao Lu*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao"> Yunpeng Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2302.07817">[arXiv]</a>
              <a href="https://github.com/wzzheng/TPVFormer">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="100px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=TPVFormer&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/TPVFormer/">[Project Page]</a>
              <a href="https://zhuanlan.zhihu.com/p/614984007">[ä¸­æ–‡è§£è¯» (in Chinese)]</a>
              <br>
              <p> Given only surround-camera motorcycle RGB images barrier as inputs, our model (trained using trailer only sparse traffic cone LiDAR point supervision) can predict the semantic occupancy for all volumes in the 3D space. </p>
            </td>
          </tr>



          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:green">3D Scene Reconstruction</h3>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/EmbodiedOcc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</papertitle>
              <br> 
              <a href="https://github.com/YkiWu">Yuqi Wu*</a>,  
              <strong>Hao Lu*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ"> Sicheng Zuo</a>, 
              <a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ"> Yuanhui Huang </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2412.04380">[arXiv]</a> 
              <a href="https://github.com/YkiWu/EmbodiedOcc">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=YkiWu&repo=EmbodiedOcc&type=star&count=true" >
              </iframe>
              <a href="https://ykiwu.github.io/EmbodiedOcc/">[Project Page]</a>
              <!-- <a href="https://zhuanlan.zhihu.com/p/12867656844">[ä¸­æ–‡è§£è¯» (in Chinese)]</a> -->
              <br>
              <p> EmbodiedOcc formulates an embodied 3D occupancy prediction task and employs a Gaussian-based framework to accomplish it.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./images/PixelGaussian.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views</papertitle>
              <br> 
              <a href="https://scholar.google.com/citations?user=r9rsD_0AAAAJ&hl=zh-CN&oi=sra"> Xin Fei </a>, 
              <strong>Hao Lu<sup>â€ </sup></strong>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://zhanwei.site/"> Wei Zhan </a>,
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>,
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2410.18979">[arXiv]</a> 
              <a href="https://github.com/Barrybarry-Smith/PixelGaussian">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=Barrybarry-Smith&repo=Driv3R&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/PixelGaussian/">[Project Page]</a>
              <br>
              <p> PixelGaussian dynamically adjusts the Gaussian distributions based on geometric complexity in a feed-forward framework. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/S3Gaussian.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>S<sup>3</sup>Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</papertitle>
              <br> 
              <a href="https://github.com/nnanhuang"> Nan Huang </a>, 
              <a href="https://ucwxb.github.io/"> Xiaobao Wei </a>, 
              <strong>Hao Lu<sup>â€ </sup></strong>, 
              <a href=""> Pengju An </a>, 
              <a href="https://lu-m13.github.io/"> Ming Lu </a>, 
              <a href="https://zhanwei.site/"> Wei Zhan </a>, 
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/"> Masayoshi Tomizuka </a>, 
              <a href="https://people.eecs.berkeley.edu/~keutzer/"> Kurt Keutzer </a>, 
              <a href="https://www.shanghangzhang.com/"> Shanghang Zhang </a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2405.20323">[arXiv]</a> 
              <a href="https://github.com/nnanhuang/S3Gaussian">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=S3Gaussian&type=star&count=true" >
              </iframe>
              <a href="https://wzzheng.net/S3Gaussian/">[Project Page]</a>
              <br>
              <p> S<sup>3</sup>Gaussian employs 3D Gaussians to model dynamic scenes for autonomous driving <b>without</b> other supervisions (e.g., 3D bounding boxes).  </p>
            </td>
          </tr>


          </tbody></table>


        
          


          <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Other topics</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="text-indent:20px;width:100%;vertical-align:middle">
            <p><heading>Other Topics</heading></p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:black">Deep Metric Learning (My Ph.D. Research Topic)</h3>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/IDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Introspective Deep Metric Learning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
              <strong>Hao Lu*<sup>â€ </sup></strong>, 
              <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=zh-CN&oi=sra"> Zheng Zhu</a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2205.04449">[arXiv]</a>
              <a href="https://github.com/wangck20/IDML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=IDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We propose an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dml-dc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Metric Learning with Adaptively Composite Dynamic Constraints</papertitle>
              <br>
              <strong>Hao Lu</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023.
              <br>
              <a href="https://cloud.tsinghua.edu.cn/f/04dfc89e8f684a0ab4a3/?dl=1">[PDF]</a>
              <!-- <a href="https://github.com/wzzheng">[Code] (coming soon)</a> -->
              <br>
              <p> This paper formulates deep metric learning under a unified framework and propose a dynamic constraint generator to produce adaptive composite constraints to train the metric towards good generalization. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/HDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Hardness-Aware Deep Metric Learning</papertitle>
              <br>
              <strong>Hao Lu</strong>, 
              <a href="https://apuaachen.github.io/Zhaodong-Chen/"> Zhaodong Chen </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019 <em>(<strong style="color:red;">oral</strong>)</em>.
              <br>
              <strong>Hao Lu</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021.
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.pdf">[PDF]</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/9035438">[PDF] (Journal version)</a>
              <a href="https://github.com/wzzheng/hdml">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=wzzheng&repo=HDML&type=star&count=true" >
              </iframe>
              <br>
              <p> We perform linear interpolation on embeddings to adaptively manipulate their hardness levels and generate corresponding label-preserving synthetics for recycled training.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DAML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Adversarial Metric Learning</papertitle>
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <strong>Hao Lu</strong>, 
              <a href="https://xudonglinthu.github.io/"> Xudong Lin </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018 <em>(<strong style="color:red;">spotlight</strong>)</em>.
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>,
              <strong>Hao Lu</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Image Processing (<strong>T-IP</strong>, IF: 11.041)</strong></em>, 2020.
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf">[PDF]</a>
              <a href="https://ieeexplore.ieee.org/abstract/document/8883191">[PDF] (Journal version)</a>
              <a href="https://github.com/anonymous1computervision/DAML">[Code]</a>
              <iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=anonymous1computervision&repo=DAML&type=star&count=true" >
              </iframe>
              <br>
              <p> We generate potential hard negatives adversarial to the learned metric as complements.</p>
            </td> 
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SDML.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structural Deep Metric Learning for Room Layout Estimation</papertitle>
              <br>
              <strong>Hao Lu</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020.
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630715.pdf">[PDF]</a>
              <br>
              <p> We are the first to apply deep metric learning to prediction tasks with structured labels.</p>
            </td> 
          </tr>

          
        
      </tbody></table>



      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <h3 style="text-indent:20px;color:black">Visual Representation Learning</h3>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="./images/SpatialFormer.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>SpatialFormer: Towards Generalizable Vision Transformers with Explicit Spatial Understanding</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=N-u2i-QAAAAJ"> Han Xiao*</a>, 
            <strong>Hao Lu*</strong>, 
            <a href="https://scholar.google.com/citations?user=11kh6C4AAAAJ">Sicheng Zuo</a>, 
            <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ">Peng Gao</a>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
            <br>
            <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024.
            <br>
            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02019.pdf">[PDF]</a>
            <!-- <a href="https://github.com/Euphoria16/TL-Align">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=nnanhuang&repo=S3Gaussian&type=star&count=true" >
            </iframe> -->
            <br>
            <p> We identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies for vision transformers. To adress this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. </p>
          </td>
        </tr>
      

      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="./images/OPERA.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions</papertitle>
          <br>
          <a href="https://scholar.google.com/citations?user=69-8jtcAAAAJ"> Chengkun Wang* </a>,
          <strong>Hao Lu*</strong>, 
          <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&amp;hl=zh-CN&amp;oi=sra"> Zheng Zhu</a>, 
          <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&amp;hl=en&amp;authuser=1"> Jie Zhou </a>,
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
          <br>
          <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
          <br>
          <a href="https://arxiv.org/abs/2210.05557">[arXiv]</a>
          <a href="https://github.com/wangck20/OPERA">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=wangck20&repo=OPERA&type=star&count=true" >
          </iframe>
          <br>
          <p> We unify fully supervised and self-supervised contrastive learning and exploit both supervisions from labeled and unlabeled data for training. </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="./images/TL-Align.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>Token-Label Alignment for Vision Transformers</papertitle>
          <br>
          <a href="https://scholar.google.com/citations?user=N-u2i-QAAAAJ&amp;hl=zh-CN&amp;oi=sra">  Han Xiao*</a>, 
          <strong>Hao Lu*</strong>, 
          <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&amp;hl=zh-CN&amp;oi=sra"> Zheng Zhu</a>, 
          <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&amp;hl=en&amp;authuser=1"> Jie Zhou </a>,
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
          <br>
          <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023.
          <br>
          <a href="https://arxiv.org/abs/2210.06455">[arXiv]</a>
          <a href="https://github.com/Euphoria16/TL-Align">[Code]</a>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=Euphoria16&repo=TL-Align&type=star&count=true" >
          </iframe>
          <br>
          <p> We identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies for vision transformers. To adress this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. </p>
        </td>
      </tr>

    </tbody></table>



      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <h3 style="text-indent:20px;color:black">Explainable Artificial Intelligence</h3>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/SAMP.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Path Choice Matters for Clear Attribution in Path Methods</papertitle>
            <br>
            <a href="http://boruizhang.site/"> Borui Zhang</a>, 
            <strong>Hao Lu</strong>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
            <br>
            <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2401.10442">[arXiv]</a>
            <a href="https://github.com/zbr17/SAMP">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=zbr17&repo=SAMP&type=star&count=true" >
            </iframe>
            <br>
            <p> To address the ambiguity in attributions caused by different path choices, we introduced the Concentration Principle and developed SAMP, an efficient model-agnostic interpreter. By incorporating the infinitesimal constraint (IC) and momentum strategy (MS), SAMP provides superior interpretations.</p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/bort.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint</papertitle>
            <br>
            <a href="http://boruizhang.site/"> Borui Zhang</a>, 
            <strong>Hao Lu</strong>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
            <br>
            <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023.
            <br>
            <a href="https://arxiv.org/abs/2212.09062">[arXiv]</a>
            <a href="https://github.com/zbr17/Bort">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=zbr17&repo=Bort&type=star&count=true" >
            </iframe>
            <br>
            <p> This paper proposes Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and transparency.</p>
          </td>
        </tr>



        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/AVSL.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Attributable Visual Similarity Learning</papertitle>
            <br>
            <a href="http://boruizhang.site/"> Borui Zhang</a>, 
            <strong>Hao Lu</strong>, 
            <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
            <br>
            <a href="https://arxiv.org/abs/2203.14932">[arXiv]</a>
            <a href="https://github.com/zbr17/DRML">[Code]</a>
            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=zbr17&repo=DRML&type=star&count=true" >
            </iframe>
            <br>
            <p> This paper proposes an attributable visual similarity learning (AVSL) framework, which employs a generalized similarity learning paradigm to represent the similarity between two images with a graph for a more accurate and explainable similarity measure between images.</p>
          </td> 
        </tr>



      </tbody></table>


      </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2024 Excellent Doctoral Dissertation of Chinese Association for Artificial Intelligence</li>
                <li style="margin: 5px;"> 2023 Tsinghua Excellent Doctoral Dissertation Award</li>
                <li style="margin: 5px;"> 2023 Beijing Outstanding Graduate</li>
                <li style="margin: 5px;"> 2023 Tsinghua Outstanding Graduate</li>
                <li style="margin: 5px;"> 2022 Xuancheng Scholarship</li>
                <li style="margin: 5px;"> 2021 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2021 CVPR Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2020 Changtong Scholarship  (highest scholarship in the Dept. of Automation)</li>
                <li style="margin: 5px;"> 2019 National Scholarship  (highest scholarship given by the government of China)</li>
                <li style="margin: 5px;"> 2017 Tung OOCL Scholarship</li>
                <li style="margin: 5px;"> 2016 German Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2019-2025, ICCV 2019-2023, ECCV 2020-2024, NeurIPS 2023-2024, ICLR 2024-2025, ICML 2025, IJCAI 2020-2024, WACV 2020-2024, ICME 2019-2024, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, T-IP, T-MM, T-CSVT, T-NNLS, T-BIOM, T-IST, Pattern Recognition, Pattern Recognition Letters
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <!-- <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3Xl5HqLv8wQcw477KsV8mPSQEnrm59hQ6peJ0jKbxdw&cl=ffffff&w=a"></script>
	  </div>         -->
	  <br>
	    &copy; Hao Lu | Last updated: Feb. 1, 2025.
</center></p>
</body>

</html>
